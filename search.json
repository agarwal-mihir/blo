[
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/2022-02-24-audio-filtering.html",
    "href": "posts/2022-02-24-audio-filtering.html",
    "title": "Audio Filtering",
    "section": "",
    "text": "Basic Imports\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport seaborn as sns\nfrom functools import partial\n\nsns.reset_defaults()\nsns.set_context(context=\"talk\", font_scale=1)\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n\n\nimport librosa\ny, sr = librosa.load(\"/Users/nipun/Downloads/external-sensors_data_audio_audacity_recorded-mask-tidal-breathing.wav\")\n\n\nplt.plot(y), sr\n\n([&lt;matplotlib.lines.Line2D at 0x139db2490&gt;], 22050)\n\n\n\n\n\n\nfrom scipy import signal\n\n\nfrom scipy.fft import fft, fftfreq\n\n\nyf = fft(y)\nxf = fftfreq(len(y), 1 / sr)\n\nplt.plot(xf, np.abs(yf), lw=0.1)\nplt.xlim((0, 1000))\n\n(0.0, 1000.0)\n\n\n\n\n\n\nplt.specgram(x = y,Fs=sr);\n#plt.ylim((0, 100))\nplt.colorbar()\n\n&lt;matplotlib.colorbar.Colorbar at 0x1424d5fd0&gt;\n\n\n\n\n\n\nsos = signal.butter(10, 20, 'lp', fs=sr, output='sos')\nfiltered = signal.sosfilt(sos, y)\nplt.plot(y)\nplt.plot(filtered)\n\n\n\n\n\nplt.plot(filtered)\n\n\n\n\n\nplt.specgram(x = filtered,Fs=sr);\n#plt.ylim((0, 100))\nplt.colorbar()\n\n&lt;matplotlib.colorbar.Colorbar at 0x1404d1d00&gt;"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesnâ€™t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/2023-06-18-Graph-Convoluntional-Layer-from-scratch.html",
    "href": "posts/2023-06-18-Graph-Convoluntional-Layer-from-scratch.html",
    "title": "Basic Imports",
    "section": "",
    "text": "---\n\nauthor: Mihir Agarwal\nbadges: true\ncategories:\n- ML\n- GNN\ndate: '2023-06-18'\noutput-file: 2023-06-18-Graph-Convoluntional-Layer-from-scratch.html\ntitle: Graph Convoluntional Layer from scratch\ntoc: true\n\n---\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport networkx as nx\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.init as init\nfrom torch_geometric.datasets import Planetoid\nimport torch.optim as optim\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\n\n\nImporting Planetoid dataset - Cora\n\ndataset = Planetoid(root='data/Planetoid', name='Cora')\n\n\ndef visualize(h, color):\n    # Perform t-SNE dimensionality reduction\n    z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n\n    # Create a scatter plot of the t-SNE embeddings\n    plt.figure(figsize=(10, 10))\n    plt.xticks([])\n    plt.yticks([])\n    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n    plt.show()\n\n\n\nVisulization of the dataset\n\nvisualize(dataset[0].x, dataset[0].y)\n\n\n\n\n\n# convert pytorch tensor to networkx graph\ndef to_networkx(data):\n    G = nx.Graph()\n    G.add_nodes_from(range(data.num_nodes))\n    G.add_edges_from(data.edge_index.t().tolist())\n    return G\n\n\nG = to_networkx(dataset[0])\n\n\n\nCreating the Adjacency Matrix\n\n# Adjacency matrix\nA = nx.adjacency_matrix(G).todense()\nA = torch.tensor(A, dtype=torch.float)\n\n\n\nCreating a GCN layer\nThe GCN layer is defined as follows:\n\\({H}^{(l+1)} = \\sigma \\left( \\mathbf{D}^{-\\frac{1}{2}} {\\mathbf{\\hat{A}}}  \\mathbf{D}^{-\\frac{1}{2}} \\mathbf{H}^{(l)} \\mathbf{W}^{(l)} \\right)\\)\nwhere \\(\\mathbf{H}^{(l)}\\) is the \\(l^{th}\\) layer of the GCN, \\(\\mathbf{A}\\) is the adjacency matrix, {} is the adjacency matrix with self-connections added, \\(\\mathbf{D}\\) is the degree matrix, and \\(\\mathbf{W}^{(l)}\\) is the weight matrix for the \\(l^{th}\\) layer.\n\nclass GCN_Layer(nn.Module):\n    def __init__(self, in_features, out_features):\n        super(GCN_Layer, self).__init__()\n        self.linear = nn.Linear(in_features=in_features, out_features=out_features)\n        \n    def gcn_layer(self, A, D):\n        A_hat1 = A + torch.eye(A.shape[0], device=A.device)\n        return torch.matmul(torch.matmul(D, A_hat1), D)\n    \n    def forward(self, A, X):\n        D = torch.diag(torch.sum(A+torch.eye(A.shape[0], device=A.device), dim=0) ** (-0.5))\n        A_hat = self.gcn_layer(A, D)\n        return F.relu(torch.matmul(A_hat, self.linear(X)))\n\n\n\nCreating the model\nThe model consists of two GCN layers and a linear layer for the output. We have used ReLU as the activation function.\n\nclass GNNModel(nn.Module):\n    def __init__(self, in_features, out_features, classes):\n        super(GNNModel, self).__init__()\n        self.layer1 = GCN_Layer(in_features, out_features)\n        self.layer2 = GCN_Layer(out_features, out_features)\n        self.linear = nn.Linear(out_features, classes)\n        \n    def forward(self, A, X):\n        H = self.layer1(A, X)\n        H = nn.ReLU()(H)\n        H = self.layer2(A, H)\n        H = nn.ReLU()(H)\n        H = self.linear(H)\n        return nn.Softmax(dim=1)(H)\n\n\n\nTraining the model\n\nX = torch.tensor(dataset[0].x, dtype=torch.float)\ny = torch.tensor(dataset[0].y, dtype=torch.long)\nin_features = X.shape[1]  # Number of input features\nhidden_dim = 64  # Number of hidden features\nclasses = 7  # Number of classes\ngcn_layer = GNNModel(in_features, hidden_dim, classes)\ncriterion = nn.CrossEntropyLoss()  # Use cross-entropy loss for classification\noptimizer = torch.optim.Adam(gcn_layer.parameters(), lr=0.01)\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    # Forward pass\n    output = gcn_layer(A, X)\n\n    # Compute the loss\n    loss = criterion(output, y)  # Assume y contains the ground truth class labels\n    \n    # Backward and optimize\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    # Compute the accuracy\n    predicted_labels = output.argmax(dim=1)\n    accuracy = (predicted_labels == y).float().mean()\n\n    # Print the loss and accuracy for monitoring\n    if (epoch + 1) % 1 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy.item():.4f}')\n\n/tmp/ipykernel_18480/3700216626.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X = torch.tensor(dataset[0].x, dtype=torch.float)\n/tmp/ipykernel_18480/3700216626.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  y = torch.tensor(dataset[0].y, dtype=torch.long)\n\n\nEpoch [1/100], Loss: 1.9451, Accuracy: 0.1544\nEpoch [2/100], Loss: 1.9375, Accuracy: 0.2585\nEpoch [3/100], Loss: 1.9232, Accuracy: 0.3386\nEpoch [4/100], Loss: 1.8973, Accuracy: 0.3032\nEpoch [5/100], Loss: 1.8595, Accuracy: 0.3024\nEpoch [6/100], Loss: 1.8221, Accuracy: 0.3021\nEpoch [7/100], Loss: 1.7975, Accuracy: 0.3024\nEpoch [8/100], Loss: 1.7573, Accuracy: 0.3619\nEpoch [9/100], Loss: 1.6980, Accuracy: 0.4996\nEpoch [10/100], Loss: 1.6558, Accuracy: 0.5971\nEpoch [11/100], Loss: 1.6148, Accuracy: 0.6300\nEpoch [12/100], Loss: 1.5620, Accuracy: 0.6713\nEpoch [13/100], Loss: 1.5196, Accuracy: 0.6961\nEpoch [14/100], Loss: 1.4932, Accuracy: 0.7212\nEpoch [15/100], Loss: 1.4673, Accuracy: 0.7430\nEpoch [16/100], Loss: 1.4392, Accuracy: 0.7633\nEpoch [17/100], Loss: 1.4154, Accuracy: 0.7722\nEpoch [18/100], Loss: 1.4006, Accuracy: 0.7762\nEpoch [19/100], Loss: 1.3909, Accuracy: 0.7766\nEpoch [20/100], Loss: 1.3782, Accuracy: 0.7884\nEpoch [21/100], Loss: 1.3642, Accuracy: 0.8209\nEpoch [22/100], Loss: 1.3541, Accuracy: 0.8401\nEpoch [23/100], Loss: 1.3437, Accuracy: 0.8431\nEpoch [24/100], Loss: 1.3326, Accuracy: 0.8468\nEpoch [25/100], Loss: 1.3253, Accuracy: 0.8490\nEpoch [26/100], Loss: 1.3207, Accuracy: 0.8479\nEpoch [27/100], Loss: 1.3164, Accuracy: 0.8527\nEpoch [28/100], Loss: 1.3126, Accuracy: 0.8545\nEpoch [29/100], Loss: 1.3102, Accuracy: 0.8564\nEpoch [30/100], Loss: 1.3076, Accuracy: 0.8586\nEpoch [31/100], Loss: 1.3045, Accuracy: 0.8634\nEpoch [32/100], Loss: 1.3019, Accuracy: 0.8648\nEpoch [33/100], Loss: 1.3002, Accuracy: 0.8671\nEpoch [34/100], Loss: 1.2982, Accuracy: 0.8674\nEpoch [35/100], Loss: 1.2962, Accuracy: 0.8693\nEpoch [36/100], Loss: 1.2948, Accuracy: 0.8704\nEpoch [37/100], Loss: 1.2928, Accuracy: 0.8719\nEpoch [38/100], Loss: 1.2893, Accuracy: 0.8741\nEpoch [39/100], Loss: 1.2820, Accuracy: 0.8848\nEpoch [40/100], Loss: 1.2654, Accuracy: 0.9066\nEpoch [41/100], Loss: 1.2517, Accuracy: 0.9206\nEpoch [42/100], Loss: 1.2573, Accuracy: 0.9147\nEpoch [43/100], Loss: 1.2534, Accuracy: 0.9202\nEpoch [44/100], Loss: 1.2437, Accuracy: 0.9287\nEpoch [45/100], Loss: 1.2401, Accuracy: 0.9306\nEpoch [46/100], Loss: 1.2376, Accuracy: 0.9321\nEpoch [47/100], Loss: 1.2363, Accuracy: 0.9324\nEpoch [48/100], Loss: 1.2352, Accuracy: 0.9350\nEpoch [49/100], Loss: 1.2325, Accuracy: 0.9376\nEpoch [50/100], Loss: 1.2295, Accuracy: 0.9394\nEpoch [51/100], Loss: 1.2271, Accuracy: 0.9413\nEpoch [52/100], Loss: 1.2255, Accuracy: 0.9420\nEpoch [53/100], Loss: 1.2252, Accuracy: 0.9424\nEpoch [54/100], Loss: 1.2252, Accuracy: 0.9424\nEpoch [55/100], Loss: 1.2231, Accuracy: 0.9457\nEpoch [56/100], Loss: 1.2214, Accuracy: 0.9468\nEpoch [57/100], Loss: 1.2211, Accuracy: 0.9468\nEpoch [58/100], Loss: 1.2209, Accuracy: 0.9468\nEpoch [59/100], Loss: 1.2200, Accuracy: 0.9479\nEpoch [60/100], Loss: 1.2188, Accuracy: 0.9490\nEpoch [61/100], Loss: 1.2185, Accuracy: 0.9494\nEpoch [62/100], Loss: 1.2184, Accuracy: 0.9501\nEpoch [63/100], Loss: 1.2179, Accuracy: 0.9501\nEpoch [64/100], Loss: 1.2170, Accuracy: 0.9505\nEpoch [65/100], Loss: 1.2165, Accuracy: 0.9505\nEpoch [66/100], Loss: 1.2164, Accuracy: 0.9505\nEpoch [67/100], Loss: 1.2162, Accuracy: 0.9505\nEpoch [68/100], Loss: 1.2159, Accuracy: 0.9509\nEpoch [69/100], Loss: 1.2156, Accuracy: 0.9509\nEpoch [70/100], Loss: 1.2153, Accuracy: 0.9516\nEpoch [71/100], Loss: 1.2151, Accuracy: 0.9520\nEpoch [72/100], Loss: 1.2148, Accuracy: 0.9520\nEpoch [73/100], Loss: 1.2147, Accuracy: 0.9520\nEpoch [74/100], Loss: 1.2145, Accuracy: 0.9516\nEpoch [75/100], Loss: 1.2143, Accuracy: 0.9520\nEpoch [76/100], Loss: 1.2142, Accuracy: 0.9520\nEpoch [77/100], Loss: 1.2140, Accuracy: 0.9520\nEpoch [78/100], Loss: 1.2139, Accuracy: 0.9520\nEpoch [79/100], Loss: 1.2137, Accuracy: 0.9520\nEpoch [80/100], Loss: 1.2136, Accuracy: 0.9520\nEpoch [81/100], Loss: 1.2134, Accuracy: 0.9524\nEpoch [82/100], Loss: 1.2132, Accuracy: 0.9531\nEpoch [83/100], Loss: 1.2130, Accuracy: 0.9535\nEpoch [84/100], Loss: 1.2128, Accuracy: 0.9535\nEpoch [85/100], Loss: 1.2126, Accuracy: 0.9531\nEpoch [86/100], Loss: 1.2126, Accuracy: 0.9531\nEpoch [87/100], Loss: 1.2125, Accuracy: 0.9538\nEpoch [88/100], Loss: 1.2125, Accuracy: 0.9538\nEpoch [89/100], Loss: 1.2124, Accuracy: 0.9535\nEpoch [90/100], Loss: 1.2123, Accuracy: 0.9535\nEpoch [91/100], Loss: 1.2123, Accuracy: 0.9535\nEpoch [92/100], Loss: 1.2122, Accuracy: 0.9535\nEpoch [93/100], Loss: 1.2121, Accuracy: 0.9535\nEpoch [94/100], Loss: 1.2120, Accuracy: 0.9538\nEpoch [95/100], Loss: 1.2120, Accuracy: 0.9538\nEpoch [96/100], Loss: 1.2119, Accuracy: 0.9538\nEpoch [97/100], Loss: 1.2118, Accuracy: 0.9542\nEpoch [98/100], Loss: 1.2117, Accuracy: 0.9542\nEpoch [99/100], Loss: 1.2116, Accuracy: 0.9542\nEpoch [100/100], Loss: 1.2115, Accuracy: 0.9542\n\n\n\noutput = gcn_layer(A, X)\ny_pred = output.argmax(dim=1)\ny = dataset[0].y\nprint(f'Accuracy of GCN model: {float(((y == y_pred).sum()) / len(y))*100}')\n\nAccuracy of GCN model: 95.42097449302673"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blogs",
    "section": "",
    "text": "Basic Imports\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJun 11, 2023\n\n\nTristan Oâ€™Malley\n\n\n\n\n\n\n  \n\n\n\n\nAudio Filtering\n\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\n\n\nFeb 24, 2022\n\n\nNipun Batra\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]