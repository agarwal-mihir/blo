[
  {
    "objectID": "posts/2023-06-18-Graph-Convoluntional-Layer-from-scratch.html",
    "href": "posts/2023-06-18-Graph-Convoluntional-Layer-from-scratch.html",
    "title": "Graph Convoluntional Layer from scratch",
    "section": "",
    "text": "In this post I am implementing Graph Convolutional Layer\n\nBasic Imports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport networkx as nx\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.init as init\nfrom torch_geometric.datasets import Planetoid\nimport torch.optim as optim\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\n\n\n\nImporting Planetoid dataset - Cora\n\ndataset = Planetoid(root='data/Planetoid', name='Cora')\n\n\ndef visualize(h, color):\n    # Perform t-SNE dimensionality reduction\n    z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n\n    # Create a scatter plot of the t-SNE embeddings\n    plt.figure(figsize=(10, 10))\n    plt.xticks([])\n    plt.yticks([])\n    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n    plt.show()\n\n\n\nVisulization of the dataset\n\nvisualize(dataset[0].x, dataset[0].y)\n\n\n\n\n\n# convert pytorch tensor to networkx graph\ndef to_networkx(data):\n    G = nx.Graph()\n    G.add_nodes_from(range(data.num_nodes))\n    G.add_edges_from(data.edge_index.t().tolist())\n    return G\n\n\nG = to_networkx(dataset[0])\n\n\n\nCreating the Adjacency Matrix\n\n# Adjacency matrix\nA = nx.adjacency_matrix(G).todense()\nA = torch.tensor(A, dtype=torch.float)\n\n\n\nCreating a GCN layer\nThe GCN layer is defined as follows:\n\\({H}^{(l+1)} = \\sigma \\left( \\mathbf{D}^{-\\frac{1}{2}} {\\mathbf{\\hat{A}}}  \\mathbf{D}^{-\\frac{1}{2}} \\mathbf{H}^{(l)} \\mathbf{W}^{(l)} \\right)\\)\nwhere \\(\\mathbf{H}^{(l)}\\) is the \\(l^{th}\\) layer of the GCN, \\(\\mathbf{A}\\) is the adjacency matrix, \\({\\mathbf{\\hat{A}}}\\) is the adjacency matrix with self-connections added, \\(\\mathbf{D}\\) is the degree matrix, and \\(\\mathbf{W}^{(l)}\\) is the weight matrix for the \\(l^{th}\\) layer.\n\nclass GCN_Layer(nn.Module):\n    def __init__(self, in_features, out_features):\n        super(GCN_Layer, self).__init__()\n        self.linear = nn.Linear(in_features=in_features, out_features=out_features)\n        \n    def gcn_layer(self, A, D):\n        A_hat1 = A + torch.eye(A.shape[0], device=A.device)\n        return torch.matmul(torch.matmul(D, A_hat1), D)\n    \n    def forward(self, A, X):\n        D = torch.diag(torch.sum(A+torch.eye(A.shape[0], device=A.device), dim=0) ** (-0.5))\n        A_hat = self.gcn_layer(A, D)\n        return F.relu(torch.matmul(A_hat, self.linear(X)))\n\n\n\nCreating the model\nThe model consists of two GCN layers and a linear layer for the output. We have used ReLU as the activation function.\n\nclass GNNModel(nn.Module):\n    def __init__(self, in_features, out_features, classes):\n        super(GNNModel, self).__init__()\n        self.layer1 = GCN_Layer(in_features, out_features)\n        self.layer2 = GCN_Layer(out_features, out_features)\n        self.linear = nn.Linear(out_features, classes)\n        \n    def forward(self, A, X):\n        H = self.layer1(A, X)\n        H = nn.ReLU()(H)\n        H = self.layer2(A, H)\n        H = nn.ReLU()(H)\n        H = self.linear(H)\n        return nn.Softmax(dim=1)(H)\n\n\n\nTraining the model\n\nX = dataset[0].x\ny = dataset[0].y\nin_features = X.shape[1]  # Number of input features\nhidden_dim = 64  # Number of hidden features\nclasses = 7  # Number of classes\ngcn_layer = GNNModel(in_features, hidden_dim, classes)\ncriterion = nn.CrossEntropyLoss()  # Use cross-entropy loss for classification\noptimizer = torch.optim.Adam(gcn_layer.parameters(), lr=0.01)\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    # Forward pass\n    output = gcn_layer(A, X)\n\n    # Compute the loss\n    loss = criterion(output, y)  # Assume y contains the ground truth class labels\n    \n    # Backward and optimize\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    # Compute the accuracy\n    predicted_labels = output.argmax(dim=1)\n    accuracy = (predicted_labels == y).float().mean()\n\n    # Print the loss and accuracy for monitoring\n    if (epoch + 1) % 1 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy.item():.4f}')\n\nEpoch [1/100], Loss: 1.9459, Accuracy: 0.0702\nEpoch [2/100], Loss: 1.9383, Accuracy: 0.3316\nEpoch [3/100], Loss: 1.9238, Accuracy: 0.3024\nEpoch [4/100], Loss: 1.8966, Accuracy: 0.3021\nEpoch [5/100], Loss: 1.8567, Accuracy: 0.3021\nEpoch [6/100], Loss: 1.8174, Accuracy: 0.3021\nEpoch [7/100], Loss: 1.7879, Accuracy: 0.3024\nEpoch [8/100], Loss: 1.7328, Accuracy: 0.4346\nEpoch [9/100], Loss: 1.6781, Accuracy: 0.5835\nEpoch [10/100], Loss: 1.6422, Accuracy: 0.5735\nEpoch [11/100], Loss: 1.5881, Accuracy: 0.6141\nEpoch [12/100], Loss: 1.5457, Accuracy: 0.6647\nEpoch [13/100], Loss: 1.5212, Accuracy: 0.6747\nEpoch [14/100], Loss: 1.5006, Accuracy: 0.6765\nEpoch [15/100], Loss: 1.4805, Accuracy: 0.6839\nEpoch [16/100], Loss: 1.4607, Accuracy: 0.6994\nEpoch [17/100], Loss: 1.4392, Accuracy: 0.7448\nEpoch [18/100], Loss: 1.4178, Accuracy: 0.7744\nEpoch [19/100], Loss: 1.4071, Accuracy: 0.7740\nEpoch [20/100], Loss: 1.4041, Accuracy: 0.7707\nEpoch [21/100], Loss: 1.3950, Accuracy: 0.7758\nEpoch [22/100], Loss: 1.3820, Accuracy: 0.7855\nEpoch [23/100], Loss: 1.3722, Accuracy: 0.7954\nEpoch [24/100], Loss: 1.3673, Accuracy: 0.7939\nEpoch [25/100], Loss: 1.3631, Accuracy: 0.7984\nEpoch [26/100], Loss: 1.3546, Accuracy: 0.8102\nEpoch [27/100], Loss: 1.3429, Accuracy: 0.8312\nEpoch [28/100], Loss: 1.3326, Accuracy: 0.8475\nEpoch [29/100], Loss: 1.3278, Accuracy: 0.8471\nEpoch [30/100], Loss: 1.3254, Accuracy: 0.8504\nEpoch [31/100], Loss: 1.3180, Accuracy: 0.8512\nEpoch [32/100], Loss: 1.3070, Accuracy: 0.8634\nEpoch [33/100], Loss: 1.2922, Accuracy: 0.8911\nEpoch [34/100], Loss: 1.2784, Accuracy: 0.9044\nEpoch [35/100], Loss: 1.2733, Accuracy: 0.9114\nEpoch [36/100], Loss: 1.2694, Accuracy: 0.9114\nEpoch [37/100], Loss: 1.2593, Accuracy: 0.9202\nEpoch [38/100], Loss: 1.2501, Accuracy: 0.9258\nEpoch [39/100], Loss: 1.2448, Accuracy: 0.9302\nEpoch [40/100], Loss: 1.2408, Accuracy: 0.9321\nEpoch [41/100], Loss: 1.2374, Accuracy: 0.9346\nEpoch [42/100], Loss: 1.2349, Accuracy: 0.9361\nEpoch [43/100], Loss: 1.2327, Accuracy: 0.9369\nEpoch [44/100], Loss: 1.2304, Accuracy: 0.9394\nEpoch [45/100], Loss: 1.2284, Accuracy: 0.9417\nEpoch [46/100], Loss: 1.2271, Accuracy: 0.9428\nEpoch [47/100], Loss: 1.2264, Accuracy: 0.9424\nEpoch [48/100], Loss: 1.2256, Accuracy: 0.9428\nEpoch [49/100], Loss: 1.2243, Accuracy: 0.9435\nEpoch [50/100], Loss: 1.2225, Accuracy: 0.9457\nEpoch [51/100], Loss: 1.2208, Accuracy: 0.9472\nEpoch [52/100], Loss: 1.2198, Accuracy: 0.9490\nEpoch [53/100], Loss: 1.2187, Accuracy: 0.9501\nEpoch [54/100], Loss: 1.2178, Accuracy: 0.9509\nEpoch [55/100], Loss: 1.2170, Accuracy: 0.9520\nEpoch [56/100], Loss: 1.2161, Accuracy: 0.9531\nEpoch [57/100], Loss: 1.2150, Accuracy: 0.9549\nEpoch [58/100], Loss: 1.2143, Accuracy: 0.9549\nEpoch [59/100], Loss: 1.2135, Accuracy: 0.9553\nEpoch [60/100], Loss: 1.2129, Accuracy: 0.9564\nEpoch [61/100], Loss: 1.2123, Accuracy: 0.9572\nEpoch [62/100], Loss: 1.2116, Accuracy: 0.9572\nEpoch [63/100], Loss: 1.2110, Accuracy: 0.9575\nEpoch [64/100], Loss: 1.2104, Accuracy: 0.9579\nEpoch [65/100], Loss: 1.2096, Accuracy: 0.9586\nEpoch [66/100], Loss: 1.2092, Accuracy: 0.9594\nEpoch [67/100], Loss: 1.2087, Accuracy: 0.9594\nEpoch [68/100], Loss: 1.2083, Accuracy: 0.9594\nEpoch [69/100], Loss: 1.2079, Accuracy: 0.9601\nEpoch [70/100], Loss: 1.2076, Accuracy: 0.9597\nEpoch [71/100], Loss: 1.2073, Accuracy: 0.9601\nEpoch [72/100], Loss: 1.2070, Accuracy: 0.9601\nEpoch [73/100], Loss: 1.2067, Accuracy: 0.9605\nEpoch [74/100], Loss: 1.2065, Accuracy: 0.9605\nEpoch [75/100], Loss: 1.2063, Accuracy: 0.9609\nEpoch [76/100], Loss: 1.2061, Accuracy: 0.9612\nEpoch [77/100], Loss: 1.2060, Accuracy: 0.9612\nEpoch [78/100], Loss: 1.2058, Accuracy: 0.9612\nEpoch [79/100], Loss: 1.2057, Accuracy: 0.9612\nEpoch [80/100], Loss: 1.2054, Accuracy: 0.9616\nEpoch [81/100], Loss: 1.2053, Accuracy: 0.9616\nEpoch [82/100], Loss: 1.2051, Accuracy: 0.9616\nEpoch [83/100], Loss: 1.2050, Accuracy: 0.9616\nEpoch [84/100], Loss: 1.2050, Accuracy: 0.9616\nEpoch [85/100], Loss: 1.2049, Accuracy: 0.9616\nEpoch [86/100], Loss: 1.2049, Accuracy: 0.9616\nEpoch [87/100], Loss: 1.2048, Accuracy: 0.9616\nEpoch [88/100], Loss: 1.2047, Accuracy: 0.9616\nEpoch [89/100], Loss: 1.2046, Accuracy: 0.9616\nEpoch [90/100], Loss: 1.2044, Accuracy: 0.9616\nEpoch [91/100], Loss: 1.2042, Accuracy: 0.9623\nEpoch [92/100], Loss: 1.2041, Accuracy: 0.9620\nEpoch [93/100], Loss: 1.2041, Accuracy: 0.9620\nEpoch [94/100], Loss: 1.2040, Accuracy: 0.9620\nEpoch [95/100], Loss: 1.2039, Accuracy: 0.9623\nEpoch [96/100], Loss: 1.2038, Accuracy: 0.9627\nEpoch [97/100], Loss: 1.2037, Accuracy: 0.9627\nEpoch [98/100], Loss: 1.2036, Accuracy: 0.9627\nEpoch [99/100], Loss: 1.2036, Accuracy: 0.9627\nEpoch [100/100], Loss: 1.2036, Accuracy: 0.9627\n\n\n\noutput = gcn_layer(A, X)\ny_pred = output.argmax(dim=1)\ny = dataset[0].y\nprint(f'Accuracy of GCN model: {float(((y == y_pred).sum()) / len(y))*100}')\n\nAccuracy of GCN model: 96.27031087875366"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blogs",
    "section": "",
    "text": "Graph Convoluntional Layer from scratch\n\n\n\n\n\n\n\nML\n\n\nGNN\n\n\n\n\n\n\n\n\n\n\n\nJun 18, 2023\n\n\nMihir Agarwal\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]