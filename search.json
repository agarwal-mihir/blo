[
  {
    "objectID": "posts/2023-08-23-Conformal-Prediction-for-Regression.html",
    "href": "posts/2023-08-23-Conformal-Prediction-for-Regression.html",
    "title": "Conformal Prediction for Regression",
    "section": "",
    "text": "Uncertainty is a pivotal factor in machine learning and predictive models. It highlights the extent of confidence and potential errors in predictions, which is essential for making well-informed decisions. Uncertainty arises due to limited data, model complexity, measurement noise, and assumptions. Recognizing uncertainty is crucial as it aids in decision-making, risk assessment, and model selection. It also guides adaptation to new data and allows for better comparison between different models. Two primary uncertainty types are epistemic (model-related) and aleatoric (data-related). Conformal prediction provides a way to quantify uncertainty by creating prediction regions with defined probabilities, enhancing the reliability of machine learning applications."
  },
  {
    "objectID": "posts/2023-08-23-Conformal-Prediction-for-Regression.html#conformal-prediction-for-general-input-and-output",
    "href": "posts/2023-08-23-Conformal-Prediction-for-Regression.html#conformal-prediction-for-general-input-and-output",
    "title": "Conformal Prediction for Regression",
    "section": "Conformal Prediction for General Input and Output",
    "text": "Conformal Prediction for General Input and Output\nConformal Prediction is a powerful framework that can be applied to general input-output scenarios, not necessarily limited to discrete outputs. The algorithm can be outlined as follows:\n\nHeuristic Notion of Uncertainty: Begin with a pre-trained model that provides predictions for input-output pairs. The model should have an associated heuristic notion of uncertainty that reflects its reliability in making predictions.\nScore Function Definition: Define a score function \\(s(x, y) \\in \\mathbb{R}\\), where larger scores indicate worse agreement between input \\(x\\) and output \\(y\\). This function captures the discrepancy between the predicted output and the true output.\nQuantile Computation: Compute the estimated quantile \\(\\hat{q}\\) as the \\((d(n+1)(1-\\alpha))\\)-th quantile of the calibration scores \\(s_1 = s(X_1, Y_1), ..., s_n = s(X_n, Y_n)\\). Here, \\(n\\) is the number of calibration instances, \\(d\\) is the number of potential outputs, and \\(\\alpha\\) controls the desired confidence level.\nPrediction Set Formation: Utilize the computed quantile to form prediction sets for new input examples \\(X_{\\text{test}}\\):\n\\[ C(X_{\\text{test}}) = \\{ y : s(X_{\\text{test}}, y) \\leq \\hat{q} \\} \\]\nThe prediction set \\(C(X_{\\text{test}})\\) contains all potential outputs \\(y\\) that have a score \\(s(X_{\\text{test}}, y)\\) within the estimated quantile \\(\\hat{q}\\). This set defines the prediction region for the new example.\n\nThis algorithm leverages the model’s score function and the quantile of calibration scores to construct prediction sets for new input examples. These prediction sets provide a measure of uncertainty and reliability associated with the model’s predictions in a wide range of scenarios.\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import trange, tqdm\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n\ndef get_simple_data_train(coef_1, coef_2, coef_3, coef_4, n_cal):\n    print(\"running simple data\")\n    # Generate data points for the custom function with some noise\n    x = np.linspace(-.2, 0.2, 300)\n    x = np.hstack([x, np.linspace(.6, 1, 300)])\n    eps = 0.1 * np.random.randn(x.shape[0])\n    y = coef_1 * np.sin(2 * np.pi*(x)) + coef_2 * np.cos(4 * np.pi *(x)) + coef_3 * x+ eps\n    x = torch.from_numpy(x).float()[:, None]\n    y = torch.from_numpy(y).float()\n    print(\"running regression data\")\n    # Split data into calibration and training sets\n    cal_idx = np.random.choice(x.shape[0], n_cal, replace=False)\n    mask = np.zeros(len(x), dtype=bool)\n    mask[cal_idx] = True\n    x_cal, y_cal = x[mask], y[mask]\n    x_train, y_train = x[~mask], y[~mask]\n    return x_train, y_train, x_cal, y_cal\n\n\ndef plot_generic(_x_train, _y_train, _x_cal, _y_cal, _add_to_plot=None, coef_1=0.3, coef_2=0.02, coef_3=0.1, coef_4=0.02):\n    # print(\"running plot_generic\")\n    fig, ax = plt.subplots(figsize=(10, 5))\n    plt.xlim([-.5, 1.5])\n    plt.ylim([-1.5, 2.5])\n    plt.xlabel(\"X\", fontsize=30)\n    plt.ylabel(\"Y\", fontsize=30)\n    plt.title(\"Plot of Training and Calibration Data with True Function\", fontsize=20)\n\n    # Generate true function curve\n    x_true = np.linspace(-.5, 1.5, 1000)\n    y_true = coef_1 * np.sin(2 * np.pi * x_true) + coef_2 * np.cos(4 * np.pi * x_true) + coef_3 * x_true\n    # print(_x_train.shape, _y_train.shape, _x_cal.shape, _y_cal.shape)\n\n    # Plot training data as green scatter points\n    ax.scatter(_x_train, _y_train, c='green', s=10, label=\"training data\")\n\n    # Plot calibration data as red scatter points\n    ax.scatter(_x_cal, _y_cal, c='red', s=10, label=\"calibration data\")\n\n    # Plot the true function as a blue line\n    ax.plot(x_true, y_true, 'b-', linewidth=3, label=\"true function\")\n\n    # If additional plot elements are provided, add them using the '_add_to_plot' function\n    if _add_to_plot is not None:\n        _add_to_plot(ax)\n\n    # Add a legend to the plot\n    plt.legend(loc='best', fontsize=15, frameon=False)\n\n    return fig, ax\n\n\ncoef_1 = 0.3\ncoef_2 = 0.3\ncoef_3 = 0.1\ncoef_4 = 0.3\nn_cal = 500\nx_train, y_train, x_cal, y_cal = get_simple_data_train(coef_1, coef_2, coef_3, coef_4, n_cal)\n\nrunning simple data\nrunning regression data\n\n\n\nplot_generic(x_train, y_train, x_cal, y_cal, coef_1=coef_1, coef_2=coef_2, coef_3=coef_3, coef_4=coef_4)\nplt.show(),\n\n\n\n\n(None,)\n\n\n\nclass MLP(nn.Module):\n    def __init__(self, input_dim=1, output_dim=1, hidden_dim=10, n_hidden_layers=1, use_dropout=False):\n        super().__init__()\n\n        self.use_dropout = use_dropout\n        if use_dropout:\n            self.dropout = nn.Dropout(p=0.5)\n        self.activation = nn.Tanh()\n\n        # dynamically define architecture\n        self.layer_sizes = [input_dim] + n_hidden_layers * [hidden_dim] + [output_dim]\n        layer_list = [nn.Linear(self.layer_sizes[idx - 1], self.layer_sizes[idx]) for idx in\n                      range(1, len(self.layer_sizes))]\n        self.layers = nn.ModuleList(layer_list)\n\n    def forward(self, input):\n        hidden = self.activation(self.layers[0](input))\n        for layer in self.layers[1:-1]:\n            hidden_temp = self.activation(layer(hidden))\n\n            if self.use_dropout:\n                hidden_temp = self.dropout(hidden_temp)\n\n            hidden = hidden_temp + hidden  # residual connection\n\n        output_mean = self.layers[-1](hidden).squeeze()\n        return output_mean\n\n\ndef train(net, train_data):\n    x_train, y_train = train_data\n    optimizer = torch.optim.Adam(params=net.parameters(), lr=1e-3)\n    criterion = nn.MSELoss()\n\n    progress_bar = trange(3000)\n    for _ in progress_bar:\n        optimizer.zero_grad()\n        loss = criterion(y_train, net(x_train))\n        progress_bar.set_postfix(loss=f'{loss / x_train.shape[0]:.3f}')\n        loss.backward()\n        optimizer.step()\n    return net\n\n\ndef plot_predictions(_x_train, _y_train, _x_cal, _y_cal, _x_test, _y_preds, coef_1=0.3, coef_2=0.02, coef_3=0.1, coef_4=0.02):\n    # print(\"running predictions\")\n    def add_predictions(ax):\n        # Plot the neural network prediction curve as a line\n        ax.plot(_x_test, _y_preds, 'y-', linewidth=3, label='neural net prediction')\n    fig, ax = plot_generic(_x_train, _y_train, _x_cal, _y_cal, add_predictions, coef_1, coef_2, coef_3, coef_4)\n    plt.title(\"Plot of Training, Calibration, and Neural Network Predictions\", fontsize=15)\n    return fig, ax\n\n\nnet = MLP(hidden_dim=30, n_hidden_layers=2)\nnet = train(net, (x_train, y_train))\n\n\n\n\n\n# compute predictions everywhere\nx_test = torch.linspace(-.5, 1.5, 1000)[:, None]\n\n\nplot_predictions(x_train, y_train, x_cal, y_cal, x_test, y_preds, coef_1=coef_1, coef_2=coef_2, coef_3=coef_3, coef_4=coef_4)\nplt.show(),\n\nNameError: name 'y_preds' is not defined"
  },
  {
    "objectID": "posts/2023-08-23-Conformal-Prediction-for-Classification.html",
    "href": "posts/2023-08-23-Conformal-Prediction-for-Classification.html",
    "title": "Conformal Prediction for Classification",
    "section": "",
    "text": "Conformal Prediction for Classification\nConformal Prediction is a versatile framework applicable to various scenarios, including classification tasks. The algorithm’s adaptation for classification is outlined as follows:\n\nHeuristic Notion of Uncertainty: Start with a pre-trained model that generates predictions for input data. The model should possess a heuristic notion of uncertainty that represents its prediction confidence.\nConformal Scores Calculation: Compute the conformal scores by applying the trained model to the calibration dataset. The socring function is\n\n\n\\[s_i=1-\\hat{\\pi}_{x_i}(y_i)\\]\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import trange, tqdm\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n\ndef get_data():\n\n    train_dataset = datasets.MNIST(root='blogs/posts/data', train=True, download=True)\n    test_dataset = datasets.MNIST(root='blogs/posts/data', train=False, download=True)\n\n    X_train, y_train = train_dataset.data.float() / 255.0, train_dataset.targets\n    X_test, y_test = test_dataset.data.float() / 255.0, test_dataset.targets\n\n    X_train = X_train.view(-1, 28*28)\n    X_test = X_test.view(-1, 28*28)\n\n    X_calib, X_train = X_train[59500:], X_train[:59500]\n    y_calib, y_train = y_train[59500:], y_train[:59500]\n\n    return X_train, y_train, X_test, y_test, X_calib, y_calib\n\n\nX_train, y_train, X_test, y_test, X_cal, y_cal = get_data()\n\n\nclass MLP(nn.Module):\n    def __init__(self):\n        super(MLP, self).__init__()\n        self.fc1 = nn.Linear(784, 32)\n        self.relu = nn.ReLU()\n        self.sigmoid1 = nn.Sigmoid()\n        self.fc2 = nn.Linear(32, 10)\n        \n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n\ndef train(_net, _train_data):\n    X_train, y_train = _train_data\n    train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(_net.parameters(), lr=0.001)\n    num_epochs = 1\n    \n    for epoch in range(num_epochs):\n        _net.train()\n        running_loss = 0.0\n        running_accuracy = 0.0\n\n        for batch_idx, (inputs, targets) in enumerate(train_loader):\n            optimizer.zero_grad()\n            outputs = _net(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n            # running_accuracy += accuracy(outputs, targets)\n  \n    return _net\n\n\nnet = MLP()\nnet = train(net, (X_train, y_train))\n\n\ny_test_pred = torch.argmax(net(X_test), dim = 1)\naccuracy = (y_test_pred == y_test).sum()/len(y_test)\nprint(f\"accuracy : {accuracy}\")\n\naccuracy : 0.9197999835014343\n\n\n\ncal_smx = torch.functional.F.softmax(net(X_calib), dim=1).detach().numpy()\nscores = 1 - cal_smx[np.arange(len(X_calib)), y_calib.numpy()]\n\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 3))\n# Plot scores of calibration data\nax[0].bar(np.arange(len(scores)), height = scores, alpha = 0.7, color = 'b')\nax[0].set_ylabel(\"Score\")\nax[0].set_xlabel(\"Calibration Data Points\")\nax[0].set_title(\"Scores of Calibration Data\")\n\n# Plot the histogram\nn, bins, _ = ax[1].hist(scores, bins=30, alpha=0.7, cumulative = True, color='#E94B3CFF', edgecolor='black', label='Score Frequency')\nax[1].set_xlabel('Scores')\nax[1].set_ylabel('Frequency')\nax[1].set_title('Histogram of Scores with Quantile Line')\nplt.show(),\n\n\n\n\n(None,)\n\n\n\nalpha = 0.1\nn_cal = 500\n\n\n\n\n\\[q = \\frac{{\\lceil (1 - \\alpha) \\cdot (n + 1) \\rceil}}{{n}}\\]\n\nq_val = np.ceil((1 - alpha) * (n_cal + 1)) / n_cal\nprint(f\"q_val: {q_val}\")\n\nq_val: 0.902\n\n\n\nq = np.quantile(scores, q_val, method=\"higher\")\nfig, ax = plt.subplots(1, 2, figsize=(12, 3))\n# Plot scores of calibration data\nax[0].bar(np.arange(len(scores)), height = scores, alpha = 0.7, color = 'b')\nax[0].set_ylabel(\"Score\")\nax[0].set_xlabel(\"Calibration Data Points\")\nax[0].set_title(\"Scores of Calibration Data\")\n\n# Plot the histogram\nn, bins, _ = ax[1].hist(scores, bins=30, alpha=0.7, cumulative = True, color='#E94B3CFF', edgecolor='black', label='Score Frequency')\n\n# Plot the vertical line at the quantile\n# q_x = np.quantile(scores, q)\nax[1].axvline(q, color='b', linestyle='dashed', linewidth=2, label=r\"Quantile (${q_{val}}$ = \" + str((\"{:.2f}\")).format(q) + \")\")\n\nax[1].set_xlabel('Scores')\nax[1].set_ylabel('Frequency')\nax[1].set_title('Histogram of Scores with Quantile Line')\nplt.legend()\nplt.show(),\n\n\n\n\n(None,)\n\n\n\nidxs = 976\n\n\ndef get_test_preds_and_smx(X_test, index, pred_sets, net, q, alpha):\n    test_smx = nn.functional.softmax(net(X_test), dim=1).detach().numpy()\n    sample_smx = test_smx[index]\n    \n    fig, axs = plt.subplots(1, 2, figsize=(12, 3))\n    axs[0].imshow(X_test[index].reshape(28,28).numpy())\n    axs[0].set_title(\"Sample test image\")\n    axs[0].set_xticks([])\n    axs[0].set_yticks([])\n    \n    axs[1].bar(range(10), sample_smx, label=\"class scores\", color = '#5B84B1FF')\n    axs[1].set_xticks(range(10))\n    axs[1].set_xticklabels([class_label(i) for i in range(10)])\n    axs[1].axhline(y=1 - q, label='threshold', color=\"#FC766AFF\", linestyle='dashed')\n    axs[1].legend(loc=1)\n    axs[1].set_title(\"Class Scores\")\n    \n    pred_set = pred_sets[index].nonzero()[0].tolist()\n    \n    return fig, axs, pred_set, get_pred_str(pred_set)\n\n\ndef class_label(i):\n    labels = {0: \"0\", 1: \"1\", 2: \"2\", 3: \"3\", 4: \"4\", \n                5: \"5\", 6: \"6\", 7: \"7\", 8: \"8\", 9: \"9\"}\n    return labels[i]\n\n\ndef get_pred_str(pred):\n    pred_str = \"{\"\n    for i in pred:\n        pred_str += class_label(i) + ', '  # Use comma instead of space\n    pred_str = pred_str.rstrip(', ') + \"}\"  # Remove the trailing comma and add closing curly brace\n    return pred_str\n\n\ndef get_pred_sets(net, test_data, q, alpha):\n    X_test, y_test = test_data\n    test_smx = nn.functional.softmax(net(X_test), dim=1).detach().numpy()\n\n    pred_sets = test_smx &gt;= (1 - q)\n    return pred_sets\n\n\npred_sets = get_pred_sets(net, (X_test, y_test), q, alpha)\n\n\nfig, ax, pred, pred_str = get_test_preds_and_smx(X_test, idxs, pred_sets, net, q, alpha)\n\n\n\n\n\nprint(pred_str)\n\n{3}"
  },
  {
    "objectID": "posts/2023-06-18-Graph-Convoluntional-Layer-from-scratch.html",
    "href": "posts/2023-06-18-Graph-Convoluntional-Layer-from-scratch.html",
    "title": "Graph Convoluntional Layer from scratch",
    "section": "",
    "text": "In this post I am implementing Graph Convolutional Layer\n\nBasic Imports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport networkx as nx\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.init as init\nfrom torch_geometric.datasets import Planetoid\nimport torch.optim as optim\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\n\n\n\nImporting Planetoid dataset - Cora\n\ndataset = Planetoid(root='data/Planetoid', name='Cora')\n\n\ndef visualize(h, color):\n    # Perform t-SNE dimensionality reduction\n    z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n\n    # Create a scatter plot of the t-SNE embeddings\n    plt.figure(figsize=(10, 10))\n    plt.xticks([])\n    plt.yticks([])\n    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n    plt.show()\n\n\n\nVisulization of the dataset\n\nvisualize(dataset[0].x, dataset[0].y)\n\n\n\n\n\n# convert pytorch tensor to networkx graph\ndef to_networkx(data):\n    G = nx.Graph()\n    G.add_nodes_from(range(data.num_nodes))\n    G.add_edges_from(data.edge_index.t().tolist())\n    return G\n\n\nG = to_networkx(dataset[0])\n\n\n\nCreating the Adjacency Matrix\n\n# Adjacency matrix\nA = nx.adjacency_matrix(G).todense()\nA = torch.tensor(A, dtype=torch.float)\n\n\n\nCreating a GCN layer\nThe GCN layer is defined as follows:\n\\({H}^{(l+1)} = \\sigma \\left( \\mathbf{D}^{-\\frac{1}{2}} {\\mathbf{\\hat{A}}}  \\mathbf{D}^{-\\frac{1}{2}} \\mathbf{H}^{(l)} \\mathbf{W}^{(l)} \\right)\\)\nwhere \\(\\mathbf{H}^{(l)}\\) is the \\(l^{th}\\) layer of the GCN, \\(\\mathbf{A}\\) is the adjacency matrix, \\({\\mathbf{\\hat{A}}}\\) is the adjacency matrix with self-connections added, \\(\\mathbf{D}\\) is the degree matrix, and \\(\\mathbf{W}^{(l)}\\) is the weight matrix for the \\(l^{th}\\) layer.\n\nclass GCN_Layer(nn.Module):\n    def __init__(self, in_features, out_features):\n        super(GCN_Layer, self).__init__()\n        self.linear = nn.Linear(in_features=in_features, out_features=out_features)\n        \n    def gcn_layer(self, A, D):\n        A_hat1 = A + torch.eye(A.shape[0], device=A.device)\n        return torch.matmul(torch.matmul(D, A_hat1), D)\n    \n    def forward(self, A, X):\n        D = torch.diag(torch.sum(A+torch.eye(A.shape[0], device=A.device), dim=0) ** (-0.5))\n        A_hat = self.gcn_layer(A, D)\n        return F.relu(torch.matmul(A_hat, self.linear(X)))\n\n\n\nCreating the model\nThe model consists of two GCN layers and a linear layer for the output. We have used ReLU as the activation function.\n\nclass GNNModel(nn.Module):\n    def __init__(self, in_features, out_features, classes):\n        super(GNNModel, self).__init__()\n        self.layer1 = GCN_Layer(in_features, out_features)\n        self.layer2 = GCN_Layer(out_features, out_features)\n        self.linear = nn.Linear(out_features, classes)\n        \n    def forward(self, A, X):\n        H = self.layer1(A, X)\n        H = nn.ReLU()(H)\n        H = self.layer2(A, H)\n        H = nn.ReLU()(H)\n        H = self.linear(H)\n        return nn.Softmax(dim=1)(H)\n\n\n\nTraining the model\n\nX = dataset[0].x\ny = dataset[0].y\nin_features = X.shape[1]  # Number of input features\nhidden_dim = 64  # Number of hidden features\nclasses = 7  # Number of classes\ngcn_layer = GNNModel(in_features, hidden_dim, classes)\ncriterion = nn.CrossEntropyLoss()  # Use cross-entropy loss for classification\noptimizer = torch.optim.Adam(gcn_layer.parameters(), lr=0.01)\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    # Forward pass\n    output = gcn_layer(A, X)\n\n    # Compute the loss\n    loss = criterion(output, y)  # Assume y contains the ground truth class labels\n    \n    # Backward and optimize\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    # Compute the accuracy\n    predicted_labels = output.argmax(dim=1)\n    accuracy = (predicted_labels == y).float().mean()\n\n    # Print the loss and accuracy for monitoring\n    if (epoch + 1) % 1 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy.item():.4f}')\n\nEpoch [1/100], Loss: 1.9459, Accuracy: 0.0702\nEpoch [2/100], Loss: 1.9383, Accuracy: 0.3316\nEpoch [3/100], Loss: 1.9238, Accuracy: 0.3024\nEpoch [4/100], Loss: 1.8966, Accuracy: 0.3021\nEpoch [5/100], Loss: 1.8567, Accuracy: 0.3021\nEpoch [6/100], Loss: 1.8174, Accuracy: 0.3021\nEpoch [7/100], Loss: 1.7879, Accuracy: 0.3024\nEpoch [8/100], Loss: 1.7328, Accuracy: 0.4346\nEpoch [9/100], Loss: 1.6781, Accuracy: 0.5835\nEpoch [10/100], Loss: 1.6422, Accuracy: 0.5735\nEpoch [11/100], Loss: 1.5881, Accuracy: 0.6141\nEpoch [12/100], Loss: 1.5457, Accuracy: 0.6647\nEpoch [13/100], Loss: 1.5212, Accuracy: 0.6747\nEpoch [14/100], Loss: 1.5006, Accuracy: 0.6765\nEpoch [15/100], Loss: 1.4805, Accuracy: 0.6839\nEpoch [16/100], Loss: 1.4607, Accuracy: 0.6994\nEpoch [17/100], Loss: 1.4392, Accuracy: 0.7448\nEpoch [18/100], Loss: 1.4178, Accuracy: 0.7744\nEpoch [19/100], Loss: 1.4071, Accuracy: 0.7740\nEpoch [20/100], Loss: 1.4041, Accuracy: 0.7707\nEpoch [21/100], Loss: 1.3950, Accuracy: 0.7758\nEpoch [22/100], Loss: 1.3820, Accuracy: 0.7855\nEpoch [23/100], Loss: 1.3722, Accuracy: 0.7954\nEpoch [24/100], Loss: 1.3673, Accuracy: 0.7939\nEpoch [25/100], Loss: 1.3631, Accuracy: 0.7984\nEpoch [26/100], Loss: 1.3546, Accuracy: 0.8102\nEpoch [27/100], Loss: 1.3429, Accuracy: 0.8312\nEpoch [28/100], Loss: 1.3326, Accuracy: 0.8475\nEpoch [29/100], Loss: 1.3278, Accuracy: 0.8471\nEpoch [30/100], Loss: 1.3254, Accuracy: 0.8504\nEpoch [31/100], Loss: 1.3180, Accuracy: 0.8512\nEpoch [32/100], Loss: 1.3070, Accuracy: 0.8634\nEpoch [33/100], Loss: 1.2922, Accuracy: 0.8911\nEpoch [34/100], Loss: 1.2784, Accuracy: 0.9044\nEpoch [35/100], Loss: 1.2733, Accuracy: 0.9114\nEpoch [36/100], Loss: 1.2694, Accuracy: 0.9114\nEpoch [37/100], Loss: 1.2593, Accuracy: 0.9202\nEpoch [38/100], Loss: 1.2501, Accuracy: 0.9258\nEpoch [39/100], Loss: 1.2448, Accuracy: 0.9302\nEpoch [40/100], Loss: 1.2408, Accuracy: 0.9321\nEpoch [41/100], Loss: 1.2374, Accuracy: 0.9346\nEpoch [42/100], Loss: 1.2349, Accuracy: 0.9361\nEpoch [43/100], Loss: 1.2327, Accuracy: 0.9369\nEpoch [44/100], Loss: 1.2304, Accuracy: 0.9394\nEpoch [45/100], Loss: 1.2284, Accuracy: 0.9417\nEpoch [46/100], Loss: 1.2271, Accuracy: 0.9428\nEpoch [47/100], Loss: 1.2264, Accuracy: 0.9424\nEpoch [48/100], Loss: 1.2256, Accuracy: 0.9428\nEpoch [49/100], Loss: 1.2243, Accuracy: 0.9435\nEpoch [50/100], Loss: 1.2225, Accuracy: 0.9457\nEpoch [51/100], Loss: 1.2208, Accuracy: 0.9472\nEpoch [52/100], Loss: 1.2198, Accuracy: 0.9490\nEpoch [53/100], Loss: 1.2187, Accuracy: 0.9501\nEpoch [54/100], Loss: 1.2178, Accuracy: 0.9509\nEpoch [55/100], Loss: 1.2170, Accuracy: 0.9520\nEpoch [56/100], Loss: 1.2161, Accuracy: 0.9531\nEpoch [57/100], Loss: 1.2150, Accuracy: 0.9549\nEpoch [58/100], Loss: 1.2143, Accuracy: 0.9549\nEpoch [59/100], Loss: 1.2135, Accuracy: 0.9553\nEpoch [60/100], Loss: 1.2129, Accuracy: 0.9564\nEpoch [61/100], Loss: 1.2123, Accuracy: 0.9572\nEpoch [62/100], Loss: 1.2116, Accuracy: 0.9572\nEpoch [63/100], Loss: 1.2110, Accuracy: 0.9575\nEpoch [64/100], Loss: 1.2104, Accuracy: 0.9579\nEpoch [65/100], Loss: 1.2096, Accuracy: 0.9586\nEpoch [66/100], Loss: 1.2092, Accuracy: 0.9594\nEpoch [67/100], Loss: 1.2087, Accuracy: 0.9594\nEpoch [68/100], Loss: 1.2083, Accuracy: 0.9594\nEpoch [69/100], Loss: 1.2079, Accuracy: 0.9601\nEpoch [70/100], Loss: 1.2076, Accuracy: 0.9597\nEpoch [71/100], Loss: 1.2073, Accuracy: 0.9601\nEpoch [72/100], Loss: 1.2070, Accuracy: 0.9601\nEpoch [73/100], Loss: 1.2067, Accuracy: 0.9605\nEpoch [74/100], Loss: 1.2065, Accuracy: 0.9605\nEpoch [75/100], Loss: 1.2063, Accuracy: 0.9609\nEpoch [76/100], Loss: 1.2061, Accuracy: 0.9612\nEpoch [77/100], Loss: 1.2060, Accuracy: 0.9612\nEpoch [78/100], Loss: 1.2058, Accuracy: 0.9612\nEpoch [79/100], Loss: 1.2057, Accuracy: 0.9612\nEpoch [80/100], Loss: 1.2054, Accuracy: 0.9616\nEpoch [81/100], Loss: 1.2053, Accuracy: 0.9616\nEpoch [82/100], Loss: 1.2051, Accuracy: 0.9616\nEpoch [83/100], Loss: 1.2050, Accuracy: 0.9616\nEpoch [84/100], Loss: 1.2050, Accuracy: 0.9616\nEpoch [85/100], Loss: 1.2049, Accuracy: 0.9616\nEpoch [86/100], Loss: 1.2049, Accuracy: 0.9616\nEpoch [87/100], Loss: 1.2048, Accuracy: 0.9616\nEpoch [88/100], Loss: 1.2047, Accuracy: 0.9616\nEpoch [89/100], Loss: 1.2046, Accuracy: 0.9616\nEpoch [90/100], Loss: 1.2044, Accuracy: 0.9616\nEpoch [91/100], Loss: 1.2042, Accuracy: 0.9623\nEpoch [92/100], Loss: 1.2041, Accuracy: 0.9620\nEpoch [93/100], Loss: 1.2041, Accuracy: 0.9620\nEpoch [94/100], Loss: 1.2040, Accuracy: 0.9620\nEpoch [95/100], Loss: 1.2039, Accuracy: 0.9623\nEpoch [96/100], Loss: 1.2038, Accuracy: 0.9627\nEpoch [97/100], Loss: 1.2037, Accuracy: 0.9627\nEpoch [98/100], Loss: 1.2036, Accuracy: 0.9627\nEpoch [99/100], Loss: 1.2036, Accuracy: 0.9627\nEpoch [100/100], Loss: 1.2036, Accuracy: 0.9627\n\n\n\noutput = gcn_layer(A, X)\ny_pred = output.argmax(dim=1)\ny = dataset[0].y\nprint(f'Accuracy of GCN model: {float(((y == y_pred).sum()) / len(y))*100}')\n\nAccuracy of GCN model: 96.27031087875366"
  },
  {
    "objectID": "posts/2023-08-23-Adaptive-Conformal-Prediction-for-Classification.html",
    "href": "posts/2023-08-23-Adaptive-Conformal-Prediction-for-Classification.html",
    "title": "Adaptive Conformal Prediction for Classification",
    "section": "",
    "text": "Adaptive Conformal Prediction for Uncertainty Estimation in Classification\nIn this section, we introduce a procedure for obtaining uncertainty estimates for classification predictions using Conformal Prediction. We start with an initial heuristic notion of uncertainty based on the softmax outputs of the model’s predictions. The goal is to refine this heuristic using the rigorous framework of conformal prediction.\n\nInitial Heuristic Uncertainty: We begin with a pre-trained model that generates predictions for a given input \\(x\\). The model provides a set of softmax probabilities \\(\\pi_1(x), \\ldots, \\pi_K(x)\\), where \\(K\\) is the number of classes. We greedily include classes in the set until we reach the true label \\(y\\), stopping at that point. This procedure, while not perfect, gives us an initial heuristic notion of uncertainty. However, we aim to transform this into a more rigorous notion.\nDefining the Score Function: We define a score function \\(s(x, y)\\) that reflects the uncertainty. The score function aggregates the softmax outputs of all classes and stops at the true label \\(y\\):\n\\[s(x, y) = \\sum_{j=1}^k \\pi_j(x) \\quad \\text{where } y = \\pi_k(x)\\]\nUnlike the score used earlier, this function considers all class probabilities, not just the true class.\nQuantile Computation: The next step is to compute the quantile \\(\\hat{q}\\) for the score function. We set \\(\\hat{q}\\) as the \\((d(n+1)(1-\\alpha))/n\\) quantile of scores \\(s_1, \\ldots, s_n\\), where \\(n\\) is the number of calibration instances and \\(d\\) is the number of potential classes.\nForming the Prediction Set: To create the prediction set, we modify the procedure slightly to avoid zero-size sets. We construct the set \\(\\pi_1(x), \\ldots, \\pi_k(x)\\) where \\(k = \\sup\\{ k' : k' = 0 \\text{ or } \\sum_{j=1}^{k'} \\pi_j(x) &lt; \\hat{q} \\} + 1\\).\n\nBy applying this conformal prediction procedure, we enhance the heuristic uncertainty into a more rigorous and reliable notion. The resulting prediction set \\(C(x)\\) provides insights into the uncertainty associated with the model’s predictions for input \\(x\\), enhancing the model’s reliability in classification tasks.\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import trange, tqdm\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n\ndef get_data():\n    train_dataset = datasets.MNIST(root='blogs/posts/data', train=True, download=True)\n    test_dataset = datasets.MNIST(root='blogs/posts/data', train=False, download=True)\n\n    X_train, y_train = train_dataset.data.float() / 255.0, train_dataset.targets\n    X_test, y_test = test_dataset.data.float() / 255.0, test_dataset.targets\n\n    X_train = X_train.view(-1, 28*28)\n    X_test = X_test.view(-1, 28*28)\n\n    X_calib, X_train = X_train[59500:], X_train[:59500]\n    y_calib, y_train = y_train[59500:], y_train[:59500]\n\n    return X_train, y_train, X_test, y_test, X_calib, y_calib\n\n\nX_train, y_train, X_test, y_test, X_calib, y_calib = get_data()\n\n\nclass MLP(nn.Module):\n    def __init__(self):\n        super(MLP, self).__init__()\n        self.fc1 = nn.Linear(784, 32)\n        self.relu = nn.ReLU()\n        self.sigmoid1 = nn.Sigmoid()\n        self.fc2 = nn.Linear(32, 10)\n        \n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n\ndef train(_net, _train_data):\n    X_train, y_train = _train_data\n    train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(_net.parameters(), lr=0.001)\n    num_epochs = 1\n    \n    for epoch in range(num_epochs):\n        _net.train()\n        running_loss = 0.0\n        running_accuracy = 0.0\n\n        for batch_idx, (inputs, targets) in enumerate(train_loader):\n            optimizer.zero_grad()\n            outputs = _net(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n  \n    return _net\n\n\nnet = MLP()\nnet = train(net, (X_train, y_train))\n\n\ny_test_pred = torch.argmax(net(X_test), dim = 1)\naccuracy = (y_test_pred == y_test).sum()/len(y_test)\nprint(f\"accuracy : {accuracy}\")\n\naccuracy : 0.9197999835014343\n\n\n\nn = 500\n\n\ncal_smx, val_smx = net(X_calib).softmax(dim = 1).detach().numpy(), net(X_test).softmax(dim = 1).detach().numpy()\ncal_labels, val_labels = y_calib.numpy(), y_test.numpy()\n\n\ncal_smx.shape\n\n(500, 10)\n\n\n\nalpha = 0.3\n\n\ncal_pi = cal_smx.argsort(1)[:, ::-1]\ncal_srt = np.take_along_axis(cal_smx, cal_pi, axis=1).cumsum(axis=1)\ncal_scores = np.take_along_axis(cal_srt, cal_pi.argsort(axis=1), axis=1)[\n    range(n), cal_labels\n]\n# Get the score quantile\n\nqhat = np.quantile(\n    cal_scores, np.ceil((n + 1) * (1 - alpha)) / n, interpolation=\"higher\"\n)\n# Deploy (output=list of length n, each element is tensor of classes)\nval_pi = val_smx.argsort(1)[:, ::-1]\nval_srt = np.take_along_axis(val_smx, val_pi, axis=1).cumsum(axis=1)\nprediction_sets = np.take_along_axis(val_srt &lt;= qhat, val_pi.argsort(axis=1), axis=1)\n\n/var/folders/jk/w0z8m7r15qz2bq604dfvr16h0000gn/T/ipykernel_6326/361661335.py:8: DeprecationWarning: the `interpolation=` argument to quantile was renamed to `method=`, which has additional options.\nUsers of the modes 'nearest', 'lower', 'higher', or 'midpoint' are encouraged to review the method they used. (Deprecated NumPy 1.22)\n  qhat = np.quantile(\n\n\n\n# Calculate empirical coverage\nempirical_coverage = prediction_sets[\n    np.arange(prediction_sets.shape[0]), val_labels\n].mean()\nprint(f\"The empirical coverage is: {empirical_coverage}\")\n\nThe empirical coverage is: 0.7293\n\n\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 3))\n# Plot scores of calibration data\nax[0].bar(np.arange(len(cal_scores)), height = cal_scores, alpha = 0.7, color = 'b')\nax[0].set_ylabel(\"Score\")\nax[0].set_xlabel(\"Calibration Data Points\")\nax[0].set_title(\"Scores of Calibration Data\")\n\n# Plot the histogram\nn, bins, _ = ax[1].hist(cal_scores, bins=30, alpha=0.7, cumulative = True, color='#E94B3CFF', edgecolor='black', label='Score Frequency')\nax[1].axvline(qhat, color='b', linestyle='dashed', linewidth=2, label=r\"Quantile (${q_{val}}$ = \" + str((\"{:.2f}\")).format(qhat) + \")\")\nax[1].set_xlabel('Scores')\nax[1].set_ylabel('Frequency')\nax[1].set_title('Histogram of Scores with Quantile Line')\nplt.show(),\n\n\n\n\n(None,)\n\n\n\ndef class_label(i):\n    labels = {0: \"0\", 1: \"1\", 2: \"2\", 3: \"3\", 4: \"4\", \n                5: \"5\", 6: \"6\", 7: \"7\", 8: \"8\", 9: \"9\"}\n    return labels[i]\n\n\ndef get_pred_str(pred):\n    pred_str = \"{\"\n    for i in pred:\n        pred_str += class_label(i) + ', '  # Use comma instead of space\n    pred_str = pred_str.rstrip(', ') + \"}\"  # Remove the trailing comma and add closing curly brace\n    return pred_str\n\n\nindex = 300\nimg_pi = val_smx[index].argsort()[::-1]\nimg_srt = np.take_along_axis(val_smx[index], img_pi, axis=0).cumsum()\nprediction_set = np.take_along_axis(img_srt &lt;= qhat, img_pi.argsort(), axis=0)\nplt.figure()\nplt.imshow(X_calib[index].reshape(28, 28), cmap=\"gray\")\nplt.axis(\"off\")\nplt.show()\nprint(f\"The prediction set is: {get_pred_str(prediction_set.nonzero()[0].tolist())}\")\n\n\n\n\nThe prediction set is: {1, 2, 3, 4, 5, 6, 8, 9}"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blogs",
    "section": "",
    "text": "Conformal Prediction for Regression\n\n\n\n\n\n\n\nML\n\n\nUncertainty\n\n\nConformal Prediction\n\n\n\n\n\n\n\n\n\n\n\nAug 23, 2023\n\n\nMihir Agarwal\n\n\n\n\n\n\n  \n\n\n\n\nConformal Prediction for Classification\n\n\n\n\n\n\n\nML\n\n\nUncertainty\n\n\nConformal Prediction\n\n\n\n\n\n\n\n\n\n\n\nAug 23, 2023\n\n\nMihir Agarwal\n\n\n\n\n\n\n  \n\n\n\n\nAdaptive Conformal Prediction for Classification\n\n\n\n\n\n\n\nML\n\n\nUncertainty\n\n\nConformal Prediction\n\n\n\n\n\n\n\n\n\n\n\nAug 23, 2023\n\n\nMihir Agarwal\n\n\n\n\n\n\n  \n\n\n\n\nGraph Convoluntional Layer from scratch\n\n\n\n\n\n\n\nML\n\n\nGNN\n\n\n\n\n\n\n\n\n\n\n\nJun 18, 2023\n\n\nMihir Agarwal\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]